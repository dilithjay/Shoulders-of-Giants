{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "import re\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aclBibPath = \"anthology.bib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(aclBibPath, \"r\", encoding=\"utf-8\") as fp:\n",
    "    bib_txt = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bibs = bib_txt.split(\"}\\n@\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_non_letters_with_spaces(input_string):\n",
    "    # Replace non-letter characters with spaces\n",
    "    result_string = re.sub(r'[^a-zA-ZÀ-ÿ ]', ' ', input_string)\n",
    "    return result_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ACL_Abstracts.txt\", \"r\") as fp:\n",
    "    abstracts = fp.read().strip().split(\"\\n\")\n",
    "with open(\"ACL_URLs.txt\", \"r\") as fp:\n",
    "    urls = fp.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_to_ignore =\"Apache,Laura,Fang,Mono,Ma,Maria,Sam,Bench,Zhuang,Male,Nara,So,Hu,Kim,Label,The,To,Yong,The,To,Adele,Are,Foma,Kaur,Bau,Kato,Dek,Naman,Dom,As,The,To,As,Dan,E,The,To,U,Even,En,Chung,Dong,Shi,Tai,Thompson,Gao,Ir,Pan,Ali,Rao,Han,Doe,Titan,Ha,Sa,Tu,Lau,Siri,Wan,She,Dai,Ding,Kang,Ge,Koch,Che,Mann,Zou,Pei,Yao,Lou,Sydney,Ju,Sha,Day,Miwa,Bai,Ko,Ga,Pal,Pe,Gun,Hung,Con,Cun,Serrano,Sui,Bu,Mehri,Od,Haji,Gal,Gey,Lui,Ho,Furu,Ak,Kao,Aro,Gen,Moro,Notre,Ido,Ron,Were,Bai,Sahu,Dem,Melo,Rama,Hunde,Dii,Yala,Sauri\".split(',')\n",
    "languages_to_ignore=languages_to_ignore+\"Uni,One,Yi,Na,Bit,Pa\".split(',') + [\"are\", \"as\", \"e\", \"en\", \"even\", \"one\", \"so\", \"to\", \"apache\", \"au\", \"u\", \"bit\", \"she\", \"siri\", \"day\", \"gun\", \"label\", \"notre\"]\n",
    "languages_to_ignore = set(languages_to_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(languages_to_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\", \".join(sorted(languages_to_ignore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_classes = {}\n",
    "for i in range(6):\n",
    "    with open(f\"LangClasses/{i}.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "        lang_classes[i] = set(fp.read().strip().split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_langs = {}\n",
    "for line in tqdm(urls):\n",
    "    title, url = line.split(\"\\t\")\n",
    "    proc_title = set(replace_non_letters_with_spaces(title.replace(\"{\", \"\").replace(\"}\", \"\")).split(\" \"))\n",
    "    for cls, langs in lang_classes.items():\n",
    "        for lang in langs:\n",
    "            if lang in languages_to_ignore:\n",
    "                continue\n",
    "            if lang in proc_title:\n",
    "                url_langs[url] = cls\n",
    "                # print(title, url, url_langs)\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in tqdm(abstracts):\n",
    "    title, url, abstract = line.split(\"\\t\")\n",
    "    proc_abs = set(replace_non_letters_with_spaces((title + \" \" +abstract).replace(\"{\", \"\").replace(\"}\", \"\")).split(\" \"))\n",
    "    for cls, langs in lang_classes.items():\n",
    "        for lang in langs:\n",
    "            if lang in languages_to_ignore:\n",
    "                continue\n",
    "            if lang in proc_abs:\n",
    "                url_langs[url] = cls\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = inflect.engine()\n",
    "\n",
    "conf=[\"first\",'second','third','fourth','fifth','sixth',\"seventh\",\"eighth\",\"ninth\",\"tenth\",\"eleventh\",\"twelfth\",\"thirteenth\",\"fourteenth\",\"fifteenth\",\"sixteenth\",\"seventeenth\",\"eighteenth\",\"nineteenth\",\"twentieth\"]\n",
    "\n",
    "capital=[c.title() for c in conf]\n",
    "\n",
    "conf=conf+capital\n",
    "\n",
    "otherVenues = set()\n",
    "\n",
    "mainVenues=[]\n",
    "\n",
    "for i in range(70):\n",
    "    conf.append(p.ordinal(i+1))\n",
    "\n",
    "conf=[\" \"+c+\" \" for c in conf]\n",
    "\n",
    "f=open(aclBibPath, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "def simplyfyVenue(x):\n",
    "  for c in conf:\n",
    "    if c in x:\n",
    "      x=x.replace(c, \"\")\n",
    "      break\n",
    "  x=x.split(\"(\")[0].strip()\n",
    "  for i in range(1900,2021):\n",
    "    if str(i) in x:\n",
    "      x=x.replace(str(i), \"\").strip()\n",
    "  return(x)\n",
    "\n",
    "mainVenues=[\"Main Conference\",\n",
    "            \"Annual Meeting of the Association for Computational Linguistics\",\n",
    "            \"North American Chapter of the Association for Computational Linguistics\",\n",
    "            \"{E}uropean Chapter of the Association for Computational Linguistics\",\n",
    "            \"Empirical Methods in Natural Language Processing\",\n",
    "            \"International Conference on Computational Linguistics\",\n",
    "            \"Conference on Computational Natural Language Learning\",\n",
    "            \"International Workshop on Semantic Evaluation\",\n",
    "            \"Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics\",\n",
    "            \"Conference on Computational Natural Language Learning\"]\n",
    "\n",
    "def mapVanue(x,isJournal):\n",
    "  global otherVenues\n",
    "  if (\"Language Resources and Evaluation\" in x) or (\"LREC\" in x):\n",
    "    return(\"LREC\")\n",
    "  elif any([v in x for v in mainVenues]):\n",
    "    mainVenues.append(x)\n",
    "    return(\"Main\")\n",
    "  elif isJournal and ((\"Transactions of the Association for Computational Linguistics\" in x) or (\"Computational Linguistics\" in x)):\n",
    "    return(\"Main\")\n",
    "  else:\n",
    "    otherVenues.add(x)\n",
    "    return(\"Other\")\n",
    "\n",
    "def defang(x):\n",
    "  s=x[x.index(\"\\\"\")+1:x.rindex(\"\\\"\")]\n",
    "  if(len(s)==0):\n",
    "    s=x[x.index(\"{\")+1:x.rindex(\"}\")]\n",
    "  return(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperDetails={}\n",
    "categories = {}\n",
    "counter=0\n",
    "for x in f:\n",
    "  if(\"booktitle\" not in x)and(\"title =\" in x or \"Title =\" in x):\n",
    "    title=defang(x)\n",
    "    counter=1\n",
    "  elif(\"booktitle\" in x):\n",
    "    place=defang(x)\n",
    "    place=mapVanue(place,False)\n",
    "    counter=counter+1\n",
    "  elif (\"journal =\" in x):\n",
    "    place=defang(x)\n",
    "    place=mapVanue(place,True)\n",
    "    counter=counter+1\n",
    "  elif(\"year =\" in x):\n",
    "    year=int(defang(x))\n",
    "    counter=counter+1\n",
    "  elif(\"url =\" in x):\n",
    "    url=defang(x)\n",
    "    counter=counter+1\n",
    "  if(counter==4):\n",
    "    paperDetails[title]=[place,year,url]\n",
    "    if year >= 2015 and url in url_langs:\n",
    "      lang_class = url_langs[url]\n",
    "      if place not in categories:\n",
    "        categories[place] = {}\n",
    "      if lang_class not in categories[place]:\n",
    "        categories[place][lang_class] = {}        \n",
    "      if year not in categories[place][lang_class]:\n",
    "        categories[place][lang_class][year] = set()\n",
    "      categories[place][lang_class][year].add(url)\n",
    "      # pdf_dir = os.path.join(save_dir, str(lang_class), str(year))\n",
    "      # url += \"\" if url.endswith(\".pdf\") else \".pdf\"\n",
    "      # if not os.path.exists(pdf_dir):\n",
    "      #   os.makedirs(pdf_dir, exist_ok=True)\n",
    "      # try:\n",
    "      #   urlretrieve(url, os.path.join(pdf_dir, os.path.basename(url)))\n",
    "      # except Exception as e:\n",
    "      #   print(url, e)\n",
    "    counter=0\n",
    "\n",
    "mainVenues=list(set(mainVenues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplyfyVenue(x):\n",
    "  for c in conf:\n",
    "    if c in x:\n",
    "      x=x.replace(c, \"\")\n",
    "  x=x.split(\"(\")[0].strip()\n",
    "  for i in range(1900,2021):\n",
    "    if str(i) in x:\n",
    "      x=x.replace(str(i), \"\").strip()\n",
    "  x = x.replace(f\"Actes de la \", \"\")\n",
    "  for i in range(30):\n",
    "    x = x.replace(\"\\\\\", \"\").replace(f\"{i}`eme \", \"\").replace(f\"Actes de la {i}e \", \"\").replace(f\"{i}e\", \"\").replace(f\"Volume {i}\", \"\").replace(f\"volume {i}\", \"\")\n",
    "  if \":\" in x:\n",
    "    x = x.split(\":\")[0]\n",
    "  return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otherVenues_simplified = set(map(lambda x: simplyfyVenue(x.replace(\"{\", \"\").replace(\"}\", \"\")), otherVenues))\n",
    "otherVenues_sorted = sorted(otherVenues_simplified)\n",
    "with open(\"other_venue_list.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(\"\\n\".join(otherVenues_sorted).replace(\"  \", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"categories.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(categories, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"categories.pkl\", \"rb\") as fp:\n",
    "    categories = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories[\"LREC\"][0][2022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place, year, url\n",
    "# ('Other', 2020, 'https://aclanthology.org/2020.findings-emnlp.425.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "samples = deepcopy(categories)\n",
    "for category in categories:\n",
    "    for cls in sorted(categories[category].keys()):\n",
    "        cls_tot = 0\n",
    "        years_sorted = sorted(categories[category][cls].keys(), key=lambda x: len(categories[category][cls][x]))\n",
    "        for i, year in enumerate(years_sorted):\n",
    "            sample_count = (20 - cls_tot) // (len(years_sorted) - i)\n",
    "            category_urls = categories[category][cls][year]\n",
    "            if len(category_urls) < sample_count:\n",
    "                samples[category][cls][year] = category_urls\n",
    "            else:\n",
    "                samples[category][cls][year] = random.sample(sorted(category_urls), k=sample_count)\n",
    "            cls_tot += len(samples[category][cls][year])\n",
    "        print(category, cls, cls_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"CategorySamplesNew/\"\n",
    "for category in samples:\n",
    "  # for lang_class in samples[category]:\n",
    "    for year in samples[category][lang_class]:\n",
    "      pdf_dir = os.path.join(save_dir, category, str(lang_class), str(year))\n",
    "      for url in samples[category][lang_class][year]:\n",
    "        url += \"\" if url.endswith(\".pdf\") else \".pdf\"\n",
    "        if not os.path.exists(pdf_dir):\n",
    "          os.makedirs(pdf_dir, exist_ok=True)\n",
    "        try:\n",
    "          urlretrieve(url, os.path.join(pdf_dir, os.path.basename(url)))\n",
    "        except Exception as e:\n",
    "          print(f\"{category}, {lang_class}, {year}, {url}\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "txts = set(map(lambda x: os.path.basename(x), glob(\"TXTs2/*/*.txt\") + glob(\"TXTs3/*/*.txt\")))\n",
    "\n",
    "seen = set()\n",
    "not_seen = set()\n",
    "\n",
    "for category in samples:\n",
    "  for lang_class in samples[category]:\n",
    "    for year in samples[category][lang_class]:\n",
    "      pdf_dir = os.path.join(save_dir, category, str(lang_class), str(year))\n",
    "      for url in samples[category][lang_class][year]:\n",
    "        url += \"\" if url.endswith(\".pdf\") else \".pdf\"\n",
    "        url = url.replace(\".pdf\", \".txt\")\n",
    "        if os.path.basename(url) in txts:\n",
    "            seen.add(os.path.basename(url))\n",
    "        else:\n",
    "            not_seen.add(os.path.basename(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seen), len(not_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"in_filtered.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(\"\\n\".join(sorted(seen)))\n",
    "with open(\"not_in_filtered.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(\"\\n\".join(sorted(not_seen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "pdfs = glob(\"PDFs1/*/*.pdf\") + glob(\"PDFs2/*/*.pdf\")\n",
    "for path in pdfs:\n",
    "    new_path = path.replace(\"PDFs1\", \"PDFs\").replace(\"PDFs2\", \"PDFs\")\n",
    "    dirname = os.path.dirname(new_path)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    shutil.copy(path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "pdf_paths = glob(\"CategorySamplesNew/*/*/*.pdf\")\n",
    "\n",
    "for pdf_path in tqdm(pdf_paths):\n",
    "    dst_path = pdf_path.replace(\".pdf\", \".txt\")\n",
    "    txt_basename = os.path.basename(dst_path)\n",
    "    txt_paths = glob(f\"TXTs/*/{txt_basename}\")\n",
    "    if txt_paths:\n",
    "        txt_path = txt_paths[0]\n",
    "        shutil.copy(txt_path, dst_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poppler_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
