{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download, Categorize, and Filter Papers\n",
    "Refer https://github.com/NisansaDdS/Some-Languages-are-More-Equal-than-Others/blob/main/Generate%20Diagrams.ipynb for downloading anthology.bib and generating ACL_Abstracts.txt and ACL_URLs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "import re\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "aclBibPath = \"anthology.bib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(aclBibPath, \"r\", encoding=\"utf-8\") as fp:\n",
    "    bib_txt = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bibs = bib_txt.split(\"}\\n@\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_non_letters_with_spaces(input_string):\n",
    "    # Replace non-letter characters with spaces\n",
    "    result_string = re.sub(r'[^a-zA-ZÀ-ÿ ]', ' ', input_string)\n",
    "    return result_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ACL_Abstracts.txt\", \"r\") as fp:\n",
    "    abstracts = fp.read().split(\"\\n\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_classes = {}\n",
    "for i in range(6):\n",
    "    with open(f\"LangClasses/{i}.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "        lang_classes[i] = set(fp.read().strip().split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_to_ignore =\"Apache,Laura,Fang,Mono,Ma,Maria,Sam,Bench,Zhuang,Male,Nara,So,Hu,Kim,Label,The,To,Yong,The,To,Adele,Are,Foma,Kaur,Bau,Kato,Dek,Naman,Dom,As,The,To,As,Dan,E,The,To,U,Even,En,Chung,Dong,Shi,Tai,Thompson,Gao,Ir,Pan,Ali,Rao,Han,Doe,Titan,Ha,Sa,Tu,Lau,Siri,Wan,She,Dai,Ding,Kang,Ge,Koch,Che,Mann,Zou,Pei,Yao,Lou,Sydney,Ju,Sha,Day,Miwa,Bai,Ko,Ga,Pal,Pe,Gun,Hung,Con,Cun,Serrano,Sui,Bu,Mehri,Od,Haji,Gal,Gey,Lui,Ho,Furu,Ak,Kao,Aro,Gen,Moro,Notre,Ido,Ron,Were,Bai,Sahu,Dem,Melo,Rama,Hunde,Dii,Yala,Sauri\".split(',')\n",
    "languages_to_ignore=languages_to_ignore+\"Uni,One,Yi,Na,Bit,Pa\".split(',') + [\"are\", \"as\", \"e\", \"en\", \"even\", \"one\", \"so\", \"to\", \"apache\", \"au\", \"u\", \"bit\", \"she\", \"siri\", \"day\", \"gun\", \"label\", \"notre\"]\n",
    "languages_to_ignore = set(languages_to_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/86948 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86948/86948 [00:09<00:00, 9285.36it/s] \n"
     ]
    }
   ],
   "source": [
    "url_cls = {}\n",
    "url_langs = {}\n",
    "for line in tqdm(abstracts):\n",
    "    title, url, abstract = line.split(\"\\t\")\n",
    "    for cls, langs in lang_classes.items():\n",
    "        proc_abs = set(replace_non_letters_with_spaces(abstract.replace(\"{\", \"\").replace(\"}\", \"\")).split(\" \"))\n",
    "        for lang in langs:\n",
    "            if lang in proc_abs and lang not in languages_to_ignore:\n",
    "                url_cls[url] = cls\n",
    "                url_langs[url] = lang\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = inflect.engine()\n",
    "\n",
    "conf=[\"first\",'second','third','fourth','fifth','sixth',\"seventh\",\"eighth\",\"ninth\",\"tenth\",\"eleventh\",\"twelfth\",\"thirteenth\",\"fourteenth\",\"fifteenth\",\"sixteenth\",\"seventeenth\",\"eighteenth\",\"nineteenth\",\"twentieth\"]\n",
    "\n",
    "capital=[c.title() for c in conf]\n",
    "\n",
    "conf=conf+capital\n",
    "\n",
    "mainVenues=[]\n",
    "\n",
    "for i in range(70):\n",
    "    conf.append(p.ordinal(i+1))\n",
    "\n",
    "conf=[\" \"+c+\" \" for c in conf]\n",
    "\n",
    "f=open(aclBibPath, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "def simplyfyVenue(x):\n",
    "  for c in conf:\n",
    "    if c in x:\n",
    "      x=x.split(c)[1]\n",
    "      break\n",
    "  x=x.split(\"(\")[0].strip()\n",
    "  for i in range(1900,2021):\n",
    "    if str(i) in x:\n",
    "      x=x.split(str(i))[1].strip()\n",
    "  return(x)\n",
    "\n",
    "mainVenues=[\"Main Conference\",\n",
    "            \"Annual Meeting of the Association for Computational Linguistics\",\n",
    "            \"North American Chapter of the Association for Computational Linguistics\",\n",
    "            \"{E}uropean Chapter of the Association for Computational Linguistics\",\n",
    "            \"Empirical Methods in Natural Language Processing\",\n",
    "            \"International Conference on Computational Linguistics\",\n",
    "            \"Conference on Computational Natural Language Learning\",\n",
    "            \"International Workshop on Semantic Evaluation\",\n",
    "            \"Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics\",\n",
    "            \"Conference on Computational Natural Language Learning\"]\n",
    "\n",
    "def mapVanue(x,isJournal):\n",
    "  if (\"Language Resources and Evaluation\" in x) or (\"LREC\" in x):\n",
    "    return(\"LREC\")\n",
    "  elif any([v in x for v in mainVenues]):\n",
    "    mainVenues.append(x)\n",
    "    return(\"Main\")\n",
    "  elif isJournal and ((\"Transactions of the Association for Computational Linguistics\" in x) or (\"Computational Linguistics\" in x)):\n",
    "    return(\"Main\")\n",
    "  else:\n",
    "    return(\"Other\")\n",
    "\n",
    "def defang(x):\n",
    "  s=x[x.index(\"\\\"\")+1:x.rindex(\"\\\"\")]\n",
    "  if(len(s)==0):\n",
    "    s=x[x.index(\"{\")+1:x.rindex(\"}\")]\n",
    "  return(s)\n",
    "\n",
    "paperDetails={}\n",
    "categories = {}\n",
    "counter=0\n",
    "\n",
    "\n",
    "for x in f:\n",
    "  if(\"booktitle\" not in x)and(\"title =\" in x or \"Title =\" in x):\n",
    "    title=defang(x)\n",
    "    counter=1\n",
    "  elif(\"booktitle\" in x):\n",
    "    place=defang(x)\n",
    "    place=mapVanue(place,False)\n",
    "    counter=counter+1\n",
    "  elif (\"journal =\" in x):\n",
    "    place=defang(x)\n",
    "    place=mapVanue(place,True)\n",
    "    counter=counter+1\n",
    "  elif(\"year =\" in x):\n",
    "    year=int(defang(x))\n",
    "    counter=counter+1\n",
    "  elif(\"url =\" in x):\n",
    "    url=defang(x)\n",
    "    counter=counter+1\n",
    "  if(counter==4):\n",
    "    paperDetails[title]=[place,year,url]\n",
    "    if year >= 2015 and url in url_cls:\n",
    "      lang_class = url_cls[url]\n",
    "      if place not in categories:\n",
    "        categories[place] = {}\n",
    "      if lang_class not in categories[place]:\n",
    "        categories[place][lang_class] = {}\n",
    "      if year not in categories[place][lang_class]:\n",
    "        categories[place][lang_class][year] = set()\n",
    "      categories[place][lang_class][year].add(url)\n",
    "    counter=0\n",
    "\n",
    "mainVenues=list(set(mainVenues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache for later use\n",
    "import pickle\n",
    "\n",
    "with open(\"categories_f.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(categories, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "samples = deepcopy(categories)\n",
    "for category in samples:\n",
    "  for lang_class in samples[category]:\n",
    "    print(category, lang_class)\n",
    "    for year in tqdm(samples[category][lang_class]):\n",
    "      save_dir = os.path.join(\"CategoryFiltered/\", category, str(lang_class), str(year))\n",
    "      for url in samples[category][lang_class][year]:\n",
    "        url += \"\" if url.endswith(\".pdf\") else \".pdf\"\n",
    "        if not os.path.exists(save_dir):\n",
    "          os.makedirs(save_dir, exist_ok=True)\n",
    "        pdf_path = os.path.join(\"PDFs\", os.path.basename(url))\n",
    "        txt_path = pdf_path.replace(\"PDFs\", \"TXTs\").replace(\".pdf\", \".txt\")\n",
    "        dst_pdf_path = os.path.join(save_dir, os.path.basename(url))\n",
    "        dst_txt_path = dst_pdf_path.replace(\".pdf\", \".txt\")\n",
    "        if os.path.exists(txt_path):\n",
    "          shutil.copy(pdf_path, dst_pdf_path)\n",
    "          shutil.copy(txt_path, dst_txt_path)\n",
    "        else:\n",
    "          try:\n",
    "            urlretrieve(url, os.path.join(save_dir, os.path.basename(url)))\n",
    "          except Exception as e:\n",
    "            print(f\"{category}, {lang_class}, {year}, {url}\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "txts = set(map(lambda x: os.path.basename(x), glob(\"TXTs/*/*.txt\")))\n",
    "\n",
    "seen = set()\n",
    "not_seen = set()\n",
    "\n",
    "for category in samples:\n",
    "  for lang_class in samples[category]:\n",
    "    for year in samples[category][lang_class]:\n",
    "      pdf_dir = os.path.join(save_dir, category, str(lang_class), str(year))\n",
    "      for url in samples[category][lang_class][year]:\n",
    "        url += \"\" if url.endswith(\".pdf\") else \".pdf\"\n",
    "        url = url.replace(\".pdf\", \".txt\")\n",
    "        if os.path.basename(url) in txts:\n",
    "            seen.add(os.path.basename(url))\n",
    "        else:\n",
    "            not_seen.add(os.path.basename(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"in_filtered.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(\"\\n\".join(sorted(seen)))\n",
    "with open(\"not_in_filtered.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(\"\\n\".join(sorted(not_seen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "pdf_papers = glob(\"PDFs/*/*.pdf\")\n",
    "txt_papers = glob(\"TXTs/*/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17931"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample filtered papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from shutil import copy\n",
    "import random\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"LangClasses/5.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    class_5_langs = fp.read().strip().split(\"\\n\")\n",
    "seen_dir = \"LangClasses/5/\" # If previously sampled papers exist\n",
    "out_dir = \"LangClasses/5-new/\" # For new sampled papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_paths = list(map(os.path.basename, glob(\"CategoryFiltered/*/5/202*/*.pdf\")))\n",
    "seen_pdfs = list(map(os.path.basename, glob(os.path.join(seen_dir, \"*.pdf\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in class_5_langs:\n",
    "    txt_paths = glob(f\"TXTs/{lang}/*.txt\")\n",
    "    sample_paths = random.sample(txt_paths, k=20)\n",
    "    for txt_path in sample_paths:\n",
    "        pdf_path = txt_path.replace(\"TXTs/\", \"PDFs/\").replace(\".txt\", \".pdf\")\n",
    "        dst_txt_path = os.path.join(out_dir, lang + \"_\" + os.path.basename(txt_path))\n",
    "        dst_pdf_path = os.path.join(out_dir, lang + \"_\" + os.path.basename(pdf_path))\n",
    "        if os.path.basename(pdf_path) in pdf_paths:\n",
    "            copy(txt_path, dst_txt_path)\n",
    "            copy(pdf_path, dst_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in class_5_langs:\n",
    "    txt_paths = glob(f\"TXTs/{lang}/*.txt\")\n",
    "    tot = 0\n",
    "    indices = list(range(len(txt_paths)))\n",
    "    seen = set()\n",
    "    while tot < 20:\n",
    "        sample_idx = random.sample(indices, k = 1)[0]\n",
    "        if sample_idx in seen:\n",
    "            continue\n",
    "        seen.add(sample_idx)\n",
    "        txt_path = txt_paths[sample_idx]\n",
    "        pdf_path = txt_path.replace(\"TXTs/\", \"PDFs/\").replace(\".txt\", \".pdf\")\n",
    "        pdf_name = os.path.basename(pdf_path)\n",
    "        dst_txt_path = os.path.join(out_dir, lang + \"_\" + os.path.basename(txt_path))\n",
    "        dst_pdf_path = os.path.join(out_dir, lang + \"_\" + pdf_name)\n",
    "        if pdf_name in pdf_paths:\n",
    "            if os.path.basename(dst_pdf_path) in seen_pdfs:\n",
    "                continue\n",
    "            copy(txt_path, dst_txt_path)\n",
    "            copy(pdf_path, dst_pdf_path)\n",
    "            tot += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample for manual verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from shutil import copy\n",
    "import random\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_paths = glob(\"PDFs/*/*.pdf\")\n",
    "sample_paths = random.sample(pdf_paths, k=100)\n",
    "out_dir = \"ManualVerifySample/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in sample_paths:\n",
    "    dst_pdf_path = os.path.join(out_dir, os.path.basename(path))\n",
    "    copy(path, dst_pdf_path)\n",
    "    \n",
    "    txt_path = path.replace(\"PDFs\", \"TXTs\").replace(\".pdf\", \".txt\")\n",
    "    if os.path.exists(txt_path):\n",
    "        dst_txt_path = os.path.join(out_dir, os.path.basename(txt_path))\n",
    "        copy(txt_path, dst_txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poppler_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
